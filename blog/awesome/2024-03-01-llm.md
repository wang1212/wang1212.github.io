---
title: 精选资源：大语言模型（LLMs）
date: 2024-03-01 14:47:00
update: 2024-05-11 22:40:00
authors: wang1212
tags: &ref_0
  - 精选资源
  - 计算机技术
  - LLMs
  - AI
keywords: *ref_0
description: 精选资源集合，有关大语言模型（LLMs）技术的资讯。
---

> _最后更新于 2024-05-11 22:40:00_

:::tip[精选资源]

这是一个系列，收集不同领域相关的精选（高价值）内容，包括深入分析文章、视频、工具等。

_探索一项新兴技术出现的背景、动机，尤其是其背后的设计哲学，更甚的是在不断的版本演进过程中遇到了什么问题，产生了什么思考，以及是如何决策并得到最优解。_

:::

自从 2022 年底 [OpenAI 发布 ChatGPT](https://openai.com/blog/chatgpt) 以来，过去的一年（2023）AI 领域再度爆发热潮，这一次**大语言模型（Large Language Model, LLMs）** 技术成为大家关注的核心。了解相关技术，探索其背后的技术原理和工程化技巧，为构建 AI 应用做好准备。

<!-- truncate -->

首先，什么是大语言模型（LLMs）？

:::note[LLM]

维基百科：

- [Large language model](https://en.wikipedia.org/wiki/Large_language_model)

偏技术性的解释：

- [Introduction to Large Language Models](https://developers.google.com/machine-learning/resources/intro-llms)
- [Large Language Models Explained](https://www.nvidia.com/en-us/glossary/large-language-models/)
- [What is a large language model (LLM)?](https://www.elastic.co/what-is/large-language-models/)

更详细的解释：

- [Large language models, explained with a minimum of math and jargon](https://www.understandingai.org/p/large-language-models-explained-with)

:::

## 提示词工程

快速上手使用类 ChatGPT 应用需要了解**提示词工程（Prompt Engineering）** 这一概念，通过不断的调整提示词来获得更好更接近预期的结果。

:::note[Prompt Engineering]

维基百科：

- [Prompt engineering](https://en.wikipedia.org/wiki/Prompt_engineering)

学习提示词工程：

- [Prompt Engineering Guide](https://www.promptingguide.ai/)
- [Prompt Engineering for Generative AI](https://developers.google.com/machine-learning/resources/prompt-eng)
  - [Prompting guide 101](https://services.google.com/fh/files/misc/gemini-for-google-workspace-prompting-guide-101.pdf)
- [Prompt engineering](https://platform.openai.com/docs/guides/prompt-engineering)

社区优秀的提示词模板案例：

- [PromptHero](https://prompthero.com/)

:::

## 机器学习

大语言模型背后是**机器学习（Machine Learning）** 领域的技术，为了更好的理解其技术原理，需要补充一些前置概念知识。

### SOTA

如何评估一个模型的好坏，State-of-the-Art(SOTA) 模型？

:::note[SOTA]

- [What is SOTA in Artificial Intelligence?](https://www.e2enetworks.com/blog/what-is-sota-in-artificial-intelligence)

:::

### Scaling Law

Scaling Law 是基于模型训练方面的实践经验的总结，代表**损失（模型性能）与模型参数数量、数据集大小和用于训练的计算量呈幂律（power-law）关系**，为大模型的设计和训练提供了理论指导。

:::note[Scaling Law]

维基百科：

- [Neural scaling law](https://en.wikipedia.org/wiki/Neural_scaling_law)

业界论文：

- [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361)

:::

### Embeddings

机器通过**嵌入（Embeddings）** 技术来理解高维数据，例如文本、图像、音频、视频等等。

:::note[Embeddings]

维基百科：

- [Word embedding](https://en.wikipedia.org/wiki/Word_embedding)

偏技术性的解释：

- [What Are Embeddings In Machine Learning?](https://aws.amazon.com/what-is/embeddings-in-machine-learning/)
- [Embeddings](https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture)

试一试：

- [Getting Started With Embeddings](https://huggingface.co/blog/getting-started-with-embeddings)

:::

### Transformer

目前，大语言模型基本上都属于 Transformer 模型，而 **Transformer 是一种基于注意力机制的神经网络（neural network）架构**，它在自然语言处理（NLP）任务中表现出色。

:::note[Transformer]

维基百科：

- [Transformer (deep learning architecture)](https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture))

更好理解一点的解释：

- [What Is a Transformer Model?](https://blogs.nvidia.com/blog/what-is-a-transformer-model/)
- [Generative AI exists because of the transformer](https://ig.ft.com/generative-ai/)

偏技术性的解释：

- [What is a Transformer?](https://huggingface.co/blog/andmholm/what-is-a-transformer)

业界论文：

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)

:::

### Fine-tune

大语言模型通常都是基于大量的数据集进行训练的预训练模型（Pre-trained models），出于保证合规的目的，为了过滤掉一些有害信息，通常都会对模型做进一步微调（fine-tune），以让其生成更符合预期的结果。同时，**微调是基于通用模型训练专有模型的重要方式**。

微调的技术有很多，列举一些比较常见的。

#### SFT

通常，语言模型的初始训练是无监督的，但微调是有监督的。**有监督微调（Supervised fine-tuning）意味着使用标记数据更新预先训练的语言模型来完成特定任务**，所使用的数据已提前检查过。

:::note[SFT]

- [What is supervised fine-tuning?](https://klu.ai/glossary/supervised-fine-tuning)
- [Fine-tuning large language models (LLMs) in 2024](https://www.superannotate.com/blog/llm-fine-tuning)
- [Understanding LLM Fine Tuning with LoRA](https://www.run.ai/guides/generative-ai/lora-fine-tuning)

偏技术性的：

- [PEFT: Parameter-Efficient Fine-Tuning of Billion-Scale Models on Low-Resource Hardware](https://huggingface.co/blog/peft)

一种特殊的微调技术，**指令调优（Instruction Tuning）**：

- [What is instruction tuning?](https://www.ibm.com/topics/instruction-tuning)

业界论文：

- [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
- [Instruction Tuning for Large Language Models: A Survey](https://arxiv.org/abs/2308.10792v5)

:::

#### RLHF

通过训练奖励模型（reward model），以**强化学习（Reinforcement Learning）** 的方式对语言模型做进一步的微调是 ChatGPT 获得成功的重要因素。

:::note[RLHF]

维基百科：

- [Reinforcement learning from human feedback](https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback)

偏技术性的解释：

- [Illustrating Reinforcement Learning from Human Feedback (RLHF)](https://huggingface.co/blog/rlhf)

:::

### MoEs

大部分大语言模型都属于密集模型（dense models），参数量级越大计算（推理）成本越高，速度越慢，消耗的硬件内存也更大，为了应对这类问题，出现了一种新的模型架构，即**混合专家（Mixture of Experts, MoEs）架构**。

:::note[MoEs]

维基百科：

- [Mixture of experts](https://en.wikipedia.org/wiki/Mixture_of_experts)

偏技术性的解释：

- [Mixture of Experts Explained](https://huggingface.co/blog/moe)
- [Applying Mixture of Experts in LLM Architectures](https://developer.nvidia.com/blog/applying-mixture-of-experts-in-llm-architectures/)

业界论文：

- [Dense Training, Sparse Inference: Rethinking Training of Mixture-of-Experts Language Models](https://arxiv.org/abs/2404.05567)

:::

最后，基于以上这些概念，可以了解下 ChatGPT 的工作原理。

> [How ChatGPT actually works](https://www.assemblyai.com/blog/how-chatgpt-actually-works/)

## 应用技术

### RAG

**检索增强生成（Retrieval-Augmented Generation, RAG）** 是一种利用从外部来源获取的事实来提高生成式 AI 模型的准确性和可靠性的技术。简单的来说，现有的大语言模型基于静态数据进行预训练，在一些对数据实时性有要求的特定场景中无法获取最新数据信息，通过 RAG 技术则可以弥补这个缺陷，通过引入外部实时数据库的方式来增强大语言模型对实时数据的响应能力。

:::note[RAG]

- [What Is Retrieval-Augmented Generation, aka RAG?](https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/)

偏技术性的解释：

- [Retrieval augmented generation: Keeping LLMs relevant and current](https://stackoverflow.blog/2023/10/18/retrieval-augmented-generation-keeping-llms-relevant-and-current/)

业界论文：

- [Retrieval-Augmented Generation for Large Language Models: A Survey](https://arxiv.org/abs/2312.10997)
- [A Survey on Retrieval-Augmented Text Generation for Large Language Models](https://arxiv.org/abs/2404.10981v1)

:::

### Agent

**人工智能代理（AI Agent）** 是一种构建 AI 应用的架构，相比于仅利用 RAG 技术能更好的解决更具体的问题，也是目前 AI 应用发展的重要趋势。

:::note[Agent]

维基百科：

- [Intelligent agent](https://en.wikipedia.org/wiki/Intelligent_agent)

一些更详细的解释：

- [What are AI Agents?](https://aws.amazon.com/what-is/ai-agents/)
- [Ai agents driving the next wave of digital transformation](https://www.leewayhertz.com/ai-agents/)

偏技术性的：

- [Introducing Agents.js: Give tools to your LLMs using JavaScript](https://huggingface.co/blog/agents-js)
- [The Anatomy of Autonomy: Why Agents are the next AI Killer App after ChatGPT](https://www.latent.space/p/agents)
- [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/)

业界论文：

- [The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey](https://arxiv.org/abs/2404.11584v1)

:::

### 开发框架

一步一步开始构建 AI 应用可能是有趣的，但也是枯燥无聊的，且需要耗费大量时间，已经有大量的相关框架工具为我们抽象了低级别的复杂细节。

:::note[开发框架]

- [LangChain](https://www.langchain.com/langchain)
- [Haystack](https://haystack.deepset.ai/)

聊天应用：

- [Lobe Chat](https://github.com/lobehub/lobe-chat)
- [Chatbot UI](https://github.com/mckaywrigley/chatbot-ui)

RAG 应用：

- [LlamaIndex](https://www.llamaindex.ai/)
- [Embedchain](https://github.com/embedchain/embedchain)

低代码应用：

- [FlowiseAI](https://flowiseai.com/)

Agent 应用：

- [@huggingface/agents](https://huggingface.co/blog/agents-js)
- [AutoGen](https://microsoft.github.io/autogen/)
- [SuperAGI](https://github.com/TransformerOptimus/SuperAGI)
- [Langroid](https://langroid.github.io/langroid/)

:::

## 社区平台

要了解大语言模型技术发展的趋势，或者寻找相关技术资源，应该对常见的一些社区平台要有所了解。

:::note[社区平台]

- [Hugging Face](https://huggingface.co/)
- [OpenAI Research](https://openai.com/research/overview)
- [Meta Llama](https://llama.meta.com/)
- [Google Gemini](https://deepmind.google/technologies/gemini/#introduction)
- [Anthropic Claude](https://www.anthropic.com/)

:::
