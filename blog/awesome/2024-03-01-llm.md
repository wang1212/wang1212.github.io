---
title: 精选资源：大语言模型（LLMs）
date: 2024-03-01 14:47:00
update: 2024-04-24 18:33:00
authors: wang1212
tags: &ref_0
  - 精选资源
  - 计算机技术
  - LLMs
  - AI
keywords: *ref_0
description: 精选资源集合，有关大语言模型（LLMs）技术的资讯。
---

> _最后更新于 2024-04-24 18:33:00_

:::tip[精选资源]

这是一个系列，收集不同领域相关的精选（高价值）内容，包括深入分析文章、视频、工具等。

_探索一项新兴技术出现的背景、动机，尤其是其背后的设计哲学，更甚的是在不断的版本演进过程中遇到了什么问题，产生了什么思考，以及是如何决策并得到最优解。_

:::

自从 2022 年底 [OpenAI 发布 ChatGPT](https://openai.com/blog/chatgpt) 以来，过去的一年（2023）AI 领域再度爆发热潮，这一次**大语言模型（Large Language Model, LLMs）**技术成为大家关注的核心。了解相关技术，探索其背后的技术原理和工程化技巧，为构建 AI 应用做好准备。

<!-- truncate -->

首先，什么是大语言模型（LLMs）？

:::note[LLM]

维基百科：

- [Large language model](https://en.wikipedia.org/wiki/Large_language_model)

偏技术性的解释：

- [Introduction to Large Language Models](https://developers.google.com/machine-learning/resources/intro-llms)
- [Large Language Models Explained](https://www.nvidia.com/en-us/glossary/large-language-models/)
- [What is a large language model (LLM)?](https://www.elastic.co/what-is/large-language-models/)

更详细的解释：

- [Large language models, explained with a minimum of math and jargon](https://www.understandingai.org/p/large-language-models-explained-with)

:::

## 提示词工程

快速上手使用类 ChatGPT 应用需要了解**提示词工程（Prompt Engineering）** 这一概念，通过不断的调整提示词来获得更好更接近预期的结果。

:::note[Prompt Engineering]

维基百科：

- [Prompt engineering](https://en.wikipedia.org/wiki/Prompt_engineering)

学习提示词工程：

- [Prompt Engineering for Generative AI](https://developers.google.com/machine-learning/resources/prompt-eng)
  - [Prompting guide 101](https://services.google.com/fh/files/misc/gemini-for-google-workspace-prompting-guide-101.pdf)
- [Prompt Engineering Guide](https://www.promptingguide.ai/)
- [Prompt engineering](https://platform.openai.com/docs/guides/prompt-engineering)

社区优秀的提示词模板案例：

- [PromptHero](https://prompthero.com/)

:::

## 机器学习

大语言模型背后是**机器学习（Machine Learning）** 领域的技术，为了更好的理解其技术原理，需要补充一些前置概念知识。

### SOTA

如何评估一个模型的好坏，State-of-the-Art(SOTA) 模型？

:::note[SOTA]

- [What is SOTA in Artificial Intelligence?](https://www.e2enetworks.com/blog/what-is-sota-in-artificial-intelligence)

:::

### Scaling Law

Scaling Law 是基于模型训练方面的实践经验的总结，代表**损失（模型性能）与模型参数数量、数据集大小和用于训练的计算量呈幂律（power-law）关系**，为大模型的设计和训练提供了理论指导。

:::note[Scaling Law]

维基百科：

- [Neural scaling law](https://en.wikipedia.org/wiki/Neural_scaling_law)

业界论文：

- [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361)

:::

### Embeddings

机器通过**嵌入（Embeddings）** 技术来理解高维数据，例如文本、图像、音频、视频等等。

:::note[Embeddings]

维基百科：

- [Word embedding](https://en.wikipedia.org/wiki/Word_embedding)

偏技术性的解释：

- [What Are Embeddings In Machine Learning?](https://aws.amazon.com/what-is/embeddings-in-machine-learning/)
- [Embeddings](https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture)

试一试：

- [Getting Started With Embeddings](https://huggingface.co/blog/getting-started-with-embeddings)

:::

### Transformer

目前，大语言模型（LLMs）基本上都属于 Transformer 模型，而 **Transformer 是一种基于注意力机制的神经网络（neural network）架构**，它在自然语言处理（NLP）任务中表现出色。

:::note[Transformer]

维基百科：

- [Transformer (deep learning architecture)](https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture))

更好理解一点的解释：

- [What Is a Transformer Model?](https://blogs.nvidia.com/blog/what-is-a-transformer-model/)
- [Generative AI exists because of the transformer](https://ig.ft.com/generative-ai/)

偏技术性的解释：

- [What is a Transformer?](https://huggingface.co/blog/andmholm/what-is-a-transformer)

:::
