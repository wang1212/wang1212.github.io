<!doctype html>
<html lang="zh-Hans" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.3.2">
<title data-rh="true">精选资源：大语言模型（LLMs） | 不如怀念</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://wang1212.github.io/2024/03/01/awesome/llm"><meta data-rh="true" property="og:locale" content="zh_Hans"><meta data-rh="true" name="docusaurus_locale" content="zh-Hans"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="zh-Hans"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="精选资源：大语言模型（LLMs） | 不如怀念"><meta data-rh="true" name="description" content="精选资源集合，有关大语言模型（LLMs）技术的资讯。"><meta data-rh="true" property="og:description" content="精选资源集合，有关大语言模型（LLMs）技术的资讯。"><meta data-rh="true" name="keywords" content="精选资源,计算机技术,AI,LLMs"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2024-03-01T14:47:00.000Z"><meta data-rh="true" property="article:author" content="https://github.com/wang1212"><meta data-rh="true" property="article:tag" content="精选资源,计算机技术,AI,LLMs"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://wang1212.github.io/2024/03/01/awesome/llm"><link data-rh="true" rel="alternate" href="https://wang1212.github.io/2024/03/01/awesome/llm" hreflang="zh-Hans"><link data-rh="true" rel="alternate" href="https://wang1212.github.io/2024/03/01/awesome/llm" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","@id":"https://wang1212.github.io/2024/03/01/awesome/llm","mainEntityOfPage":"https://wang1212.github.io/2024/03/01/awesome/llm","url":"https://wang1212.github.io/2024/03/01/awesome/llm","headline":"精选资源：大语言模型（LLMs）","name":"精选资源：大语言模型（LLMs）","description":"精选资源集合，有关大语言模型（LLMs）技术的资讯。","datePublished":"2024-03-01T14:47:00.000Z","author":{"@type":"Person","name":"不如怀念","description":"Web 前端工程师 (Web Front-end Engineer)","url":"https://github.com/wang1212","email":"mrwang1212@126.com","image":"/img/authors/wang1212.png"},"keywords":["精选资源","计算机技术","AI","LLMs"],"isPartOf":{"@type":"Blog","@id":"https://wang1212.github.io/","name":"Blog"}}</script><link rel="alternate" type="application/rss+xml" href="/rss.xml" title="不如怀念 RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/atom.xml" title="不如怀念 Atom Feed"><link rel="stylesheet" href="/assets/css/styles.14a4682a.css">
<script src="/assets/js/runtime~main.5b14ccd2.js" defer="defer"></script>
<script src="/assets/js/main.f4784fc1.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="跳到主要内容"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">跳到主要内容</a></div><nav aria-label="主导航" class="navbar navbar--fixed-top navbarHideable_m1mJ"><div class="navbar__inner"><div class="navbar__items"><button aria-label="切换导航栏" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><b class="navbar__title text--truncate">不如怀念</b></a><a class="navbar__item navbar__link" href="/archive">归档</a><a class="navbar__item navbar__link" href="/record">记录</a><a class="navbar__item navbar__link" href="/timeline">历程</a></div><div class="navbar__items navbar__items--right"><a href="https://wang1212.github.io/awesome-favorites-list" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">精选资源<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="切换浅色/暗黑模式（当前为浅色模式）" aria-label="切换浅色/暗黑模式（当前为浅色模式）" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="搜索" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="最近博文导航"><div class="sidebarItemTitle_pO2u margin-bottom--md">近期文章</div><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/2024/03/22/ai/llm/agent">谈谈现阶段对 AI Agent 的理解</a></li><li class="sidebarItem__DBe"><a aria-current="page" class="sidebarItemLink_mo7H sidebarItemLinkActive_I1ZP" href="/2024/03/01/awesome/llm">精选资源：大语言模型（LLMs）</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/2023/12/16/life/tourism/tourism-shaoxing">绍兴鲁迅故居之行</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/2023/11/10/computer-technology/web-frontend/morph-animation">解析 ECharts 的 UniversalTransition 动画</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/2023/08/27/life/tourism/tourism-nanjing">此行南京</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/2023/07/22/tools/notes-app">谈谈笔记记录和知识管理工具</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/2023/06/19/computer-technology/ai/ai-knowledge-base">结合 AI 技术构建可视化知识库的尝试</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/2023/05/28/computer-technology/nodejs/nodejs-loader">通过 Node.js 自定义加载器运行代码</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/computer-technology/web-frontend/api/webgpu-gpgpu">WebGPU – Web 平台的通用计算 API</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/computer-technology/3d/forward-rendering-and-deferred-rendering">3D 开发：正向渲染与延迟渲染</a></li></ul></nav></aside><main class="col col--7"><article><header><h1 class="title_f1Hy">精选资源：大语言模型（LLMs）</h1><div class="container_mt6G margin-vert--md"><time datetime="2024-03-01T14:47:00.000Z">2024年3月1日</time> · <!-- -->阅读需 9 分钟</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/wang1212" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="/img/authors/wang1212.png" alt="不如怀念"></a><div class="avatar__intro"><div class="avatar__name"><a href="https://github.com/wang1212" target="_blank" rel="noopener noreferrer"><span>不如怀念</span></a></div><small class="avatar__subtitle">Web 前端工程师 (Web Front-end Engineer)</small></div></div></div></div></header><div id="__blog-post-container" class="markdown"><blockquote>
<p><em>最后更新于 2024-05-11 22:40:00</em></p>
</blockquote>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>精选资源</div><div class="admonitionContent_BuS1"><p>这是一个系列，收集不同领域相关的精选（高价值）内容，包括深入分析文章、视频、工具等。</p><p><em>探索一项新兴技术出现的背景、动机，尤其是其背后的设计哲学，更甚的是在不断的版本演进过程中遇到了什么问题，产生了什么思考，以及是如何决策并得到最优解。</em></p></div></div>
<p>自从 2022 年底 <a href="https://openai.com/blog/chatgpt" target="_blank" rel="noopener noreferrer">OpenAI 发布 ChatGPT</a> 以来，过去的一年（2023）AI 领域再度爆发热潮，这一次<strong>大语言模型（Large Language Model, LLMs）</strong> 技术成为大家关注的核心。了解相关技术，探索其背后的技术原理和工程化技巧，为构建 AI 应用做好准备。</p>
<p>首先，什么是大语言模型（LLMs）？</p>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>LLM</div><div class="admonitionContent_BuS1"><p>维基百科：</p><ul>
<li><a href="https://en.wikipedia.org/wiki/Large_language_model" target="_blank" rel="noopener noreferrer">Large language model</a></li>
</ul><p>偏技术性的解释：</p><ul>
<li><a href="https://developers.google.com/machine-learning/resources/intro-llms" target="_blank" rel="noopener noreferrer">Introduction to Large Language Models</a></li>
<li><a href="https://www.nvidia.com/en-us/glossary/large-language-models/" target="_blank" rel="noopener noreferrer">Large Language Models Explained</a></li>
<li><a href="https://www.elastic.co/what-is/large-language-models/" target="_blank" rel="noopener noreferrer">What is a large language model (LLM)?</a></li>
</ul><p>更详细的解释：</p><ul>
<li><a href="https://www.understandingai.org/p/large-language-models-explained-with" target="_blank" rel="noopener noreferrer">Large language models, explained with a minimum of math and jargon</a></li>
</ul></div></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="提示词工程">提示词工程<a href="#提示词工程" class="hash-link" aria-label="提示词工程的直接链接" title="提示词工程的直接链接">​</a></h2>
<p>快速上手使用类 ChatGPT 应用需要了解<strong>提示词工程（Prompt Engineering）</strong> 这一概念，通过不断的调整提示词来获得更好更接近预期的结果。</p>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>Prompt Engineering</div><div class="admonitionContent_BuS1"><p>维基百科：</p><ul>
<li><a href="https://en.wikipedia.org/wiki/Prompt_engineering" target="_blank" rel="noopener noreferrer">Prompt engineering</a></li>
</ul><p>学习提示词工程：</p><ul>
<li><a href="https://www.promptingguide.ai/" target="_blank" rel="noopener noreferrer">Prompt Engineering Guide</a></li>
<li><a href="https://developers.google.com/machine-learning/resources/prompt-eng" target="_blank" rel="noopener noreferrer">Prompt Engineering for Generative AI</a>
<ul>
<li><a href="https://services.google.com/fh/files/misc/gemini-for-google-workspace-prompting-guide-101.pdf" target="_blank" rel="noopener noreferrer">Prompting guide 101</a></li>
</ul>
</li>
<li><a href="https://platform.openai.com/docs/guides/prompt-engineering" target="_blank" rel="noopener noreferrer">Prompt engineering</a></li>
</ul><p>社区优秀的提示词模板案例：</p><ul>
<li><a href="https://prompthero.com/" target="_blank" rel="noopener noreferrer">PromptHero</a></li>
</ul></div></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="机器学习">机器学习<a href="#机器学习" class="hash-link" aria-label="机器学习的直接链接" title="机器学习的直接链接">​</a></h2>
<p>大语言模型背后是<strong>机器学习（Machine Learning）</strong> 领域的技术，为了更好的理解其技术原理，需要补充一些前置概念知识。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="sota">SOTA<a href="#sota" class="hash-link" aria-label="SOTA的直接链接" title="SOTA的直接链接">​</a></h3>
<p>如何评估一个模型的好坏，State-of-the-Art(SOTA) 模型？</p>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>SOTA</div><div class="admonitionContent_BuS1"><ul>
<li><a href="https://www.e2enetworks.com/blog/what-is-sota-in-artificial-intelligence" target="_blank" rel="noopener noreferrer">What is SOTA in Artificial Intelligence?</a></li>
</ul></div></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="scaling-law">Scaling Law<a href="#scaling-law" class="hash-link" aria-label="Scaling Law的直接链接" title="Scaling Law的直接链接">​</a></h3>
<p>Scaling Law 是基于模型训练方面的实践经验的总结，代表<strong>损失（模型性能）与模型参数数量、数据集大小和用于训练的计算量呈幂律（power-law）关系</strong>，为大模型的设计和训练提供了理论指导。</p>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>Scaling Law</div><div class="admonitionContent_BuS1"><p>维基百科：</p><ul>
<li><a href="https://en.wikipedia.org/wiki/Neural_scaling_law" target="_blank" rel="noopener noreferrer">Neural scaling law</a></li>
</ul><p>业界论文：</p><ul>
<li><a href="https://arxiv.org/abs/2001.08361" target="_blank" rel="noopener noreferrer">Scaling Laws for Neural Language Models</a></li>
</ul></div></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="embeddings">Embeddings<a href="#embeddings" class="hash-link" aria-label="Embeddings的直接链接" title="Embeddings的直接链接">​</a></h3>
<p>机器通过<strong>嵌入（Embeddings）</strong> 技术来理解高维数据，例如文本、图像、音频、视频等等。</p>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>Embeddings</div><div class="admonitionContent_BuS1"><p>维基百科：</p><ul>
<li><a href="https://en.wikipedia.org/wiki/Word_embedding" target="_blank" rel="noopener noreferrer">Word embedding</a></li>
</ul><p>偏技术性的解释：</p><ul>
<li><a href="https://aws.amazon.com/what-is/embeddings-in-machine-learning/" target="_blank" rel="noopener noreferrer">What Are Embeddings In Machine Learning?</a></li>
<li><a href="https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture" target="_blank" rel="noopener noreferrer">Embeddings</a></li>
</ul><p>试一试：</p><ul>
<li><a href="https://huggingface.co/blog/getting-started-with-embeddings" target="_blank" rel="noopener noreferrer">Getting Started With Embeddings</a></li>
</ul></div></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="transformer">Transformer<a href="#transformer" class="hash-link" aria-label="Transformer的直接链接" title="Transformer的直接链接">​</a></h3>
<p>目前，大语言模型基本上都属于 Transformer 模型，而 <strong>Transformer 是一种基于注意力机制的神经网络（neural network）架构</strong>，它在自然语言处理（NLP）任务中表现出色。</p>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>Transformer</div><div class="admonitionContent_BuS1"><p>维基百科：</p><ul>
<li><a href="https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)" target="_blank" rel="noopener noreferrer">Transformer (deep learning architecture)</a></li>
</ul><p>更好理解一点的解释：</p><ul>
<li><a href="https://blogs.nvidia.com/blog/what-is-a-transformer-model/" target="_blank" rel="noopener noreferrer">What Is a Transformer Model?</a></li>
<li><a href="https://ig.ft.com/generative-ai/" target="_blank" rel="noopener noreferrer">Generative AI exists because of the transformer</a></li>
</ul><p>偏技术性的解释：</p><ul>
<li><a href="https://huggingface.co/blog/andmholm/what-is-a-transformer" target="_blank" rel="noopener noreferrer">What is a Transformer?</a></li>
</ul><p>业界论文：</p><ul>
<li><a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener noreferrer">Attention Is All You Need</a></li>
</ul></div></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="fine-tune">Fine-tune<a href="#fine-tune" class="hash-link" aria-label="Fine-tune的直接链接" title="Fine-tune的直接链接">​</a></h3>
<p>大语言模型通常都是基于大量的数据集进行训练的预训练模型（Pre-trained models），出于保证合规的目的，为了过滤掉一些有害信息，通常都会对模型做进一步微调（fine-tune），以让其生成更符合预期的结果。同时，<strong>微调是基于通用模型训练专有模型的重要方式</strong>。</p>
<p>微调的技术有很多，列举一些比较常见的。</p>
<h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="sft">SFT<a href="#sft" class="hash-link" aria-label="SFT的直接链接" title="SFT的直接链接">​</a></h4>
<p>通常，语言模型的初始训练是无监督的，但微调是有监督的。<strong>有监督微调（Supervised fine-tuning）意味着使用标记数据更新预先训练的语言模型来完成特定任务</strong>，所使用的数据已提前检查过。</p>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>SFT</div><div class="admonitionContent_BuS1"><ul>
<li><a href="https://klu.ai/glossary/supervised-fine-tuning" target="_blank" rel="noopener noreferrer">What is supervised fine-tuning?</a></li>
<li><a href="https://www.superannotate.com/blog/llm-fine-tuning" target="_blank" rel="noopener noreferrer">Fine-tuning large language models (LLMs) in 2024</a></li>
<li><a href="https://www.run.ai/guides/generative-ai/lora-fine-tuning" target="_blank" rel="noopener noreferrer">Understanding LLM Fine Tuning with LoRA</a></li>
</ul><p>偏技术性的：</p><ul>
<li><a href="https://huggingface.co/blog/peft" target="_blank" rel="noopener noreferrer">PEFT: Parameter-Efficient Fine-Tuning of Billion-Scale Models on Low-Resource Hardware</a></li>
</ul><p>一种特殊的微调技术，<strong>指令调优（Instruction Tuning）</strong>：</p><ul>
<li><a href="https://www.ibm.com/topics/instruction-tuning" target="_blank" rel="noopener noreferrer">What is instruction tuning?</a></li>
</ul><p>业界论文：</p><ul>
<li><a href="https://arxiv.org/abs/2106.09685" target="_blank" rel="noopener noreferrer">LoRA: Low-Rank Adaptation of Large Language Models</a></li>
<li><a href="https://arxiv.org/abs/2308.10792v5" target="_blank" rel="noopener noreferrer">Instruction Tuning for Large Language Models: A Survey</a></li>
</ul></div></div>
<h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="rlhf">RLHF<a href="#rlhf" class="hash-link" aria-label="RLHF的直接链接" title="RLHF的直接链接">​</a></h4>
<p>通过训练奖励模型（reward model），以<strong>强化学习（Reinforcement Learning）</strong> 的方式对语言模型做进一步的微调是 ChatGPT 获得成功的重要因素。</p>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>RLHF</div><div class="admonitionContent_BuS1"><p>维基百科：</p><ul>
<li><a href="https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback" target="_blank" rel="noopener noreferrer">Reinforcement learning from human feedback</a></li>
</ul><p>偏技术性的解释：</p><ul>
<li><a href="https://huggingface.co/blog/rlhf" target="_blank" rel="noopener noreferrer">Illustrating Reinforcement Learning from Human Feedback (RLHF)</a></li>
</ul></div></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="moes">MoEs<a href="#moes" class="hash-link" aria-label="MoEs的直接链接" title="MoEs的直接链接">​</a></h3>
<p>大部分大语言模型都属于密集模型（dense models），参数量级越大计算（推理）成本越高，速度越慢，消耗的硬件内存也更大，为了应对这类问题，出现了一种新的模型架构，即<strong>混合专家（Mixture of Experts, MoEs）架构</strong>。</p>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>MoEs</div><div class="admonitionContent_BuS1"><p>维基百科：</p><ul>
<li><a href="https://en.wikipedia.org/wiki/Mixture_of_experts" target="_blank" rel="noopener noreferrer">Mixture of experts</a></li>
</ul><p>偏技术性的解释：</p><ul>
<li><a href="https://huggingface.co/blog/moe" target="_blank" rel="noopener noreferrer">Mixture of Experts Explained</a></li>
<li><a href="https://developer.nvidia.com/blog/applying-mixture-of-experts-in-llm-architectures/" target="_blank" rel="noopener noreferrer">Applying Mixture of Experts in LLM Architectures</a></li>
</ul><p>业界论文：</p><ul>
<li><a href="https://arxiv.org/abs/2404.05567" target="_blank" rel="noopener noreferrer">Dense Training, Sparse Inference: Rethinking Training of Mixture-of-Experts Language Models</a></li>
</ul></div></div>
<p>最后，基于以上这些概念，可以了解下 ChatGPT 的工作原理。</p>
<blockquote>
<p><a href="https://www.assemblyai.com/blog/how-chatgpt-actually-works/" target="_blank" rel="noopener noreferrer">How ChatGPT actually works</a></p>
</blockquote>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="应用技术">应用技术<a href="#应用技术" class="hash-link" aria-label="应用技术的直接链接" title="应用技术的直接链接">​</a></h2>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="rag">RAG<a href="#rag" class="hash-link" aria-label="RAG的直接链接" title="RAG的直接链接">​</a></h3>
<p><strong>检索增强生成（Retrieval-Augmented Generation, RAG）</strong> 是一种利用从外部来源获取的事实来提高生成式 AI 模型的准确性和可靠性的技术。简单的来说，现有的大语言模型基于静态数据进行预训练，在一些对数据实时性有要求的特定场景中无法获取最新数据信息，通过 RAG 技术则  可以弥补这个缺陷，通过引入外部实时数据库的方式来增强大语言模型对实时数据的响应能力。</p>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>RAG</div><div class="admonitionContent_BuS1"><ul>
<li><a href="https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/" target="_blank" rel="noopener noreferrer">What Is Retrieval-Augmented Generation, aka RAG?</a></li>
</ul><p>偏技术性的解释：</p><ul>
<li><a href="https://stackoverflow.blog/2023/10/18/retrieval-augmented-generation-keeping-llms-relevant-and-current/" target="_blank" rel="noopener noreferrer">Retrieval augmented generation: Keeping LLMs relevant and current</a></li>
</ul><p>业界论文：</p><ul>
<li><a href="https://arxiv.org/abs/2312.10997" target="_blank" rel="noopener noreferrer">Retrieval-Augmented Generation for Large Language Models: A Survey</a></li>
<li><a href="https://arxiv.org/abs/2404.10981v1" target="_blank" rel="noopener noreferrer">A Survey on Retrieval-Augmented Text Generation for Large Language Models</a></li>
</ul></div></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="agent">Agent<a href="#agent" class="hash-link" aria-label="Agent的直接链接" title="Agent的直接链接">​</a></h3>
<p><strong>人工智能代理（AI Agent）</strong> 是一种构建 AI 应用的架构，相比于仅利用 RAG 技术能更好的解决更具体的问题，也是目前 AI 应用发展的重要趋势。</p>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>Agent</div><div class="admonitionContent_BuS1"><p>维基百科：</p><ul>
<li><a href="https://en.wikipedia.org/wiki/Intelligent_agent" target="_blank" rel="noopener noreferrer">Intelligent agent</a></li>
</ul><p>一些更详细的解释：</p><ul>
<li><a href="https://aws.amazon.com/what-is/ai-agents/" target="_blank" rel="noopener noreferrer">What are AI Agents?</a></li>
<li><a href="https://www.leewayhertz.com/ai-agents/" target="_blank" rel="noopener noreferrer">Ai agents driving the next wave of digital transformation</a></li>
</ul><p>偏技术性的：</p><ul>
<li><a href="https://huggingface.co/blog/agents-js" target="_blank" rel="noopener noreferrer">Introducing Agents.js: Give tools to your LLMs using JavaScript</a></li>
<li><a href="https://www.latent.space/p/agents" target="_blank" rel="noopener noreferrer">The Anatomy of Autonomy: Why Agents are the next AI Killer App after ChatGPT</a></li>
<li><a href="https://lilianweng.github.io/posts/2023-06-23-agent/" target="_blank" rel="noopener noreferrer">LLM Powered Autonomous Agents</a></li>
</ul><p>业界论文：</p><ul>
<li><a href="https://arxiv.org/abs/2404.11584v1" target="_blank" rel="noopener noreferrer">The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey</a></li>
</ul></div></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="开发框架">开发框架<a href="#开发框架" class="hash-link" aria-label="开发框架的直接链接" title="开发框架的直接链接">​</a></h3>
<p>一步一步开始构建 AI 应用可能是有趣的，但也是枯燥无聊的，且需要耗费大量时间，已经有大量的相关框架工具为我们抽象了低级别的复杂细节。</p>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>开发框架</div><div class="admonitionContent_BuS1"><ul>
<li><a href="https://www.langchain.com/langchain" target="_blank" rel="noopener noreferrer">LangChain</a></li>
<li><a href="https://haystack.deepset.ai/" target="_blank" rel="noopener noreferrer">Haystack</a></li>
</ul><p>聊天应用：</p><ul>
<li><a href="https://github.com/lobehub/lobe-chat" target="_blank" rel="noopener noreferrer">Lobe Chat</a></li>
<li><a href="https://github.com/mckaywrigley/chatbot-ui" target="_blank" rel="noopener noreferrer">Chatbot UI</a></li>
</ul><p>RAG 应用：</p><ul>
<li><a href="https://www.llamaindex.ai/" target="_blank" rel="noopener noreferrer">LlamaIndex</a></li>
<li><a href="https://github.com/embedchain/embedchain" target="_blank" rel="noopener noreferrer">Embedchain</a></li>
</ul><p>低代码应用：</p><ul>
<li><a href="https://flowiseai.com/" target="_blank" rel="noopener noreferrer">FlowiseAI</a></li>
</ul><p>Agent 应用：</p><ul>
<li><a href="https://huggingface.co/blog/agents-js" target="_blank" rel="noopener noreferrer">@huggingface/agents</a></li>
<li><a href="https://microsoft.github.io/autogen/" target="_blank" rel="noopener noreferrer">AutoGen</a></li>
<li><a href="https://github.com/TransformerOptimus/SuperAGI" target="_blank" rel="noopener noreferrer">SuperAGI</a></li>
<li><a href="https://langroid.github.io/langroid/" target="_blank" rel="noopener noreferrer">Langroid</a></li>
</ul></div></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="社区平台">社区平台<a href="#社区平台" class="hash-link" aria-label="社区平台的直接链接" title="社区平台的直接链接">​</a></h2>
<p>要了解大语言模型技术发展的趋势，或者寻找相关技术资源，应该对常见的一些社区平台要有所了解。</p>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>社区平台</div><div class="admonitionContent_BuS1"><ul>
<li><a href="https://huggingface.co/" target="_blank" rel="noopener noreferrer">Hugging Face</a></li>
<li><a href="https://openai.com/research/overview" target="_blank" rel="noopener noreferrer">OpenAI Research</a></li>
<li><a href="https://llama.meta.com/" target="_blank" rel="noopener noreferrer">Meta Llama</a></li>
<li><a href="https://deepmind.google/technologies/gemini/#introduction" target="_blank" rel="noopener noreferrer">Google Gemini</a></li>
<li><a href="https://www.anthropic.com/" target="_blank" rel="noopener noreferrer">Anthropic Claude</a></li>
</ul></div></div></div><footer class="docusaurus-mt-lg"><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class="col"><b>标签：</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/精选资源">精选资源</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/计算机技术">计算机技术</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/ai">AI</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/ll-ms">LLMs</a></li></ul></div></div><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class="col"><a href="https://github.com/wang1212/wang1212.github.io/tree/master/blog/awesome/2024-03-01-llm.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>编辑此页</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="博文分页导航"><a class="pagination-nav__link pagination-nav__link--prev" href="/2024/03/22/ai/llm/agent"><div class="pagination-nav__sublabel">较新一篇</div><div class="pagination-nav__label">谈谈现阶段对 AI Agent 的理解</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/2023/12/16/life/tourism/tourism-shaoxing"><div class="pagination-nav__sublabel"> 较旧一篇</div><div class="pagination-nav__label">绍兴鲁迅故居之行</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#提示词工程" class="table-of-contents__link toc-highlight">提示词工程</a></li><li><a href="#机器学习" class="table-of-contents__link toc-highlight">机器学习</a><ul><li><a href="#sota" class="table-of-contents__link toc-highlight">SOTA</a></li><li><a href="#scaling-law" class="table-of-contents__link toc-highlight">Scaling Law</a></li><li><a href="#embeddings" class="table-of-contents__link toc-highlight">Embeddings</a></li><li><a href="#transformer" class="table-of-contents__link toc-highlight">Transformer</a></li><li><a href="#fine-tune" class="table-of-contents__link toc-highlight">Fine-tune</a></li><li><a href="#moes" class="table-of-contents__link toc-highlight">MoEs</a></li></ul></li><li><a href="#应用技术" class="table-of-contents__link toc-highlight">应用技术</a><ul><li><a href="#rag" class="table-of-contents__link toc-highlight">RAG</a></li><li><a href="#agent" class="table-of-contents__link toc-highlight">Agent</a></li><li><a href="#开发框架" class="table-of-contents__link toc-highlight">开发框架</a></li></ul></li><li><a href="#社区平台" class="table-of-contents__link toc-highlight">社区平台</a></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">关于我</div><ul class="footer__items clean-list"><li class="footer__item"><a href="mailto:mrwang1212@126.com" target="_blank" rel="noopener noreferrer" class="footer__link-item">Email<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://github.com/wang1212" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://gitee.com/i_wang1212" target="_blank" rel="noopener noreferrer" class="footer__link-item">Gitee<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">更多</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://wang1212.github.io/awesome-favorites-list" target="_blank" rel="noopener noreferrer" class="footer__link-item">Awesome Favorites List<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://wang1212.github.io/the-book-of-ruby" target="_blank" rel="noopener noreferrer" class="footer__link-item">The Book Of Ruby (ZH)<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://wang1212.github.io/echarts-api-docs" target="_blank" rel="noopener noreferrer" class="footer__link-item">Apache ECharts APIs Guide<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 My Blog. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>